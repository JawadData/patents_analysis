{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWpUpxX6nEQH",
        "outputId": "0d66479d-de2d-41ce-c65f-90b45d352298"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "!pip install langdetect translate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KMdplHWqt8pa"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import to_date, lit\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Lecture de CSV avec Spark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df_wipo = spark.read.csv(\"/content/big_data_project.wipo_end.csv\", header=True)\n",
        "df_es = spark.read.csv(\"/content/big_data_project.es.csv\", header=True)\n",
        "df_cana = spark.read.csv(\"/content/big_data_project.cn.csv\", header=True)\n",
        "df_fpo = spark.read.csv(\"/content/big_data_project.fpn.csv\", header=True)\n",
        "df_gp = spark.read.csv(\"/content/big_data_project.gp.csv\", header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSnbvWjXuDma",
        "outputId": "aa1c1a64-d178-4dd2-dc68-9a77aeb40207"
      },
      "outputs": [],
      "source": [
        "df_fpo.show(10, truncate= False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7XDWqy-0G0w"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWLDu_vfmFCf",
        "outputId": "58cb33f9-996e-4943-e84b-5ae6f69ddf2f"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import to_date, lit\n",
        "\n",
        "\n",
        "df_wipo = df_wipo.withColumn(\"Application_Date\", to_date(\"Application_Date\", \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\n",
        "df_es = df_es.withColumn(\"Date_application\", to_date(\"Date_application\", \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\n",
        "df_cana = df_cana.withColumn(\"Filled Date\", to_date(\"Filled Date\", \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\n",
        "df_fpo = df_fpo.withColumn(\"Filing Date\", to_date(\"Filing Date\", \"MM/dd/yyyy\"))\n",
        "df_gp = df_gp.withColumn(\"Filing Date\", to_date(\"Filing Date\", \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\n",
        "\n",
        "df_wipo_titles = df_wipo.select(\"Title\", \"Abstract\", \"Inventors\", \"Application_Date\", \"Office\").withColumn(\"Source\", lit(\"Wipo\"))\n",
        "df_es_titles = df_es.select(\"Title\", \"Patent_abstract\", \"Inventors\", \"Date_application\", \"Country_Name\").withColumn(\"Source\", lit(\"Es\"))\n",
        "df_cana_titles = df_cana.select(\"Title\", \"Abstract\", \"Inventors\", \"Filled Date\", \"Country\").withColumn(\"Source\", lit(\"Cana\"))\n",
        "df_fpo_titles = df_fpo.select(\"Title\", \"Summary\", \"Inventors\", \"Filing Date\", \"Country\").withColumn(\"Source\", lit(\"Fpo\"))\n",
        "df_gp_titles = df_gp.select(\"Title\", \"Abstract\", \"Inventors\", \"Filing Date\", \"Country Name\").withColumn(\"Source\", lit(\"Gp\"))\n",
        "\n",
        "dfs_merged = df_wipo_titles.union(df_es_titles).union(df_cana_titles).union(df_fpo_titles).union(df_gp_titles)\n",
        "\n",
        "dfs_merged = dfs_merged.withColumnRenamed(\"Office\", \"country\")\n",
        "\n",
        "dfs_merged.show(3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1Lk3A0wVqMn",
        "outputId": "3dfc5721-d509-4467-fdfa-67b0c45923eb"
      },
      "outputs": [],
      "source": [
        "\n",
        "count_before = dfs_merged.count()\n",
        "\n",
        "dfs_merged = dfs_merged.dropDuplicates()\n",
        "\n",
        "count_after = dfs_merged.count()\n",
        "\n",
        "if count_before == count_after:\n",
        "    print(\"Il n'y a pas de duplicatas.\")\n",
        "else:\n",
        "    print(\"Il y a des duplicatas et ils ont été supprimés.\")\n",
        "    print(f\"Nombre de lignes avant suppression : {count_before}\")\n",
        "    print(f\"Nombre de lignes après suppression : {count_after}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFqUSqLZVRRo",
        "outputId": "f50d1939-569d-4481-de78-c7cd16990cf8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "spark = SparkSession.builder.appName(\"IndexingRows\").getOrCreate()\n",
        "\n",
        "rdd = dfs_merged.rdd.zipWithIndex()\n",
        "\n",
        "dfs_merged = rdd.map(lambda row: Row(*row[0], row[1])).toDF(dfs_merged.columns + [\"row_index\"])\n",
        "\n",
        "dfs_merged.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "rsCqtVqsaqtU",
        "outputId": "d285e982-69e4-4c96-e277-1c47f42d6c9b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_pandas = dfs_merged.toPandas()\n",
        "\n",
        "df_pandas.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uQpsC7qy2OkT",
        "outputId": "9aee06bc-b1ab-458e-d56d-69beb95cef7f"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lower, regexp_replace, isnull, explode\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"WordCloudExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "dfs_merged = dfs_merged.filter(col(\"Title\").isNotNull())\n",
        "dfs_merged = dfs_merged.filter(col(\"Abstract\").isNotNull())\n",
        "\n",
        "dfs_merged = dfs_merged.withColumn(\"title_cleaned\", lower(col(\"Title\")))\n",
        "dfs_merged = dfs_merged.withColumn(\"title_cleaned\", regexp_replace(col(\"title_cleaned\"), \"[^a-zA-Z\\\\s]\", \"\"))\n",
        "tokenizer_title = Tokenizer(inputCol=\"title_cleaned\", outputCol=\"title_tokens\")\n",
        "dfs_merged = tokenizer_title.transform(dfs_merged)\n",
        "remover_title = StopWordsRemover(inputCol=\"title_tokens\", outputCol=\"title_no_stopwords\", stopWords=StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
        "dfs_merged = remover_title.transform(dfs_merged)\n",
        "remover_fr_title = StopWordsRemover(inputCol=\"title_no_stopwords\", outputCol=\"title_no_stopwords_fr\", stopWords=StopWordsRemover.loadDefaultStopWords(\"french\"))\n",
        "dfs_merged = remover_fr_title.transform(dfs_merged)\n",
        "\n",
        "dfs_merged = dfs_merged.withColumn(\"abstract_cleaned\", lower(col(\"Abstract\")))\n",
        "dfs_merged = dfs_merged.withColumn(\"abstract_cleaned\", regexp_replace(col(\"abstract_cleaned\"), \"[^a-zA-Z\\\\s]\", \"\"))\n",
        "tokenizer_abstract = Tokenizer(inputCol=\"abstract_cleaned\", outputCol=\"abstract_tokens\")\n",
        "dfs_merged = tokenizer_abstract.transform(dfs_merged)\n",
        "remover_abstract = StopWordsRemover(inputCol=\"abstract_tokens\", outputCol=\"abstract_no_stopwords\", stopWords=StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
        "dfs_merged = remover_abstract.transform(dfs_merged)\n",
        "remover_fr_abstract = StopWordsRemover(inputCol=\"abstract_no_stopwords\", outputCol=\"abstract_no_stopwords_fr\", stopWords=StopWordsRemover.loadDefaultStopWords(\"french\"))\n",
        "dfs_merged = remover_fr_abstract.transform(dfs_merged)\n",
        "\n",
        "def generate_wordcloud(df=None, col_name=None, title=None):\n",
        "    words_df = df.select(explode(col(col_name)).alias(\"word\"))\n",
        "    word_counts = words_df.groupBy(\"word\").count()\n",
        "    word_counts_dict = {row['word']: row['count'] for row in word_counts.collect()}\n",
        "\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis', max_words=100, max_font_size=120).generate_from_frequencies(word_counts_dict)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(title, fontsize=20, pad=20)\n",
        "    plt.show()\n",
        "\n",
        "generate_wordcloud(dfs_merged, \"title_no_stopwords_fr\", \"Nuage de Mots pour la Colonne 'Title' \")\n",
        "\n",
        "generate_wordcloud(dfs_merged, \"abstract_no_stopwords_fr\", \"Nuage de Mots pour la Colonne 'Abstract'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lejtQ9yPuVvQ"
      },
      "outputs": [],
      "source": [
        "dfs_merged_fin = dfs_merged.select('Title', 'Abstract', 'Inventors', 'Application_Date', 'country', 'Source','title_no_stopwords_fr', 'abstract_no_stopwords_fr')\n",
        "\n",
        "dfs_merged_fin = dfs_merged_fin.toPandas()\n",
        "nouvelles_colonnes = {\n",
        "    'title_no_stopwords_fr': 'title_tokens',\n",
        "    'abstract_no_stopwords_fr': 'abstract_tokens'\n",
        "}\n",
        "\n",
        "dfs_merged_fin = dfs_merged_fin.rename(columns=nouvelles_colonnes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb_Zj-n0x5Ja"
      },
      "outputs": [],
      "source": [
        "\n",
        "csv_path = \"/content/df_merged.csv\"\n",
        "dfs_merged_fin.to_csv(csv_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc8oOoBMSiag"
      },
      "outputs": [],
      "source": [
        "dfs_merged_pandas = dfs_merged.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "yWCjzq66w305",
        "outputId": "a1a2816a-8424-4d48-9300-11b5048a43ee"
      },
      "outputs": [],
      "source": [
        "dfs_merged_pandas.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O6Vsr8SUxbo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "df_inventors = dfs_merged_pandas[['Inventors']].copy()\n",
        "df_inventors['row_index'] = df_inventors.index\n",
        "\n",
        "def split_inventors(df):\n",
        " \n",
        "    rows = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        if pd.notna(row['Inventors']): \n",
        "            inventors = re.split(r'[\\s,]*and[\\s,]*|[\\s,]*,[\\s,]*', row['Inventors'].lower())\n",
        "            for inventor in inventors:\n",
        "\n",
        "                inventor = re.sub(r'\\[.*?\\]', '', inventor.strip())\n",
        "                if inventor: \n",
        "                    rows.append({'Inventor': inventor, 'row_index': row['row_index']})\n",
        "\n",
        "    new_df = pd.DataFrame(rows)\n",
        "    return new_df\n",
        "\n",
        "df_result = split_inventors(df_inventors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jumQDm8IxIBt",
        "outputId": "2d929abc-597b-4663-9be7-1b71b137d265"
      },
      "outputs": [],
      "source": [
        "df_result.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPe2_7QgybSf"
      },
      "outputs": [],
      "source": [
        "\n",
        "excel_path = \"/content/inventors.xlsx\"\n",
        "df_result.to_excel(excel_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "j903IATGypQa",
        "outputId": "236089fd-f7cf-49dd-94b2-39d6f6bd92e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "occurrences_df = df_result['Inventor'].value_counts().reset_index()\n",
        "occurrences_df.columns = ['Inventor', 'Occurrences']\n",
        "occurrences_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Huamp-yuzFp_"
      },
      "outputs": [],
      "source": [
        "\n",
        "excel_path = \"/content/inventors_ocurrence.xlsx\"\n",
        "occurrences_df.to_excel(excel_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCgtxQgLzxPZ",
        "outputId": "2acc1721-d098-42ff-fad6-d12e795705e8"
      },
      "outputs": [],
      "source": [
        "dfs_merged.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APeigjdRX0ox",
        "outputId": "2d0f7ab9-89c5-46f9-83fa-0dfff74b6a5f"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "from langdetect import detect\n",
        "from translate import Translator\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TitleTranslation\").getOrCreate()\n",
        "\n",
        "\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "detect_language_udf = udf(detect_language, StringType())\n",
        "\n",
        "dfs_merged = dfs_merged.withColumn(\"language_Title\", detect_language_udf(col(\"Title\")))\n",
        "\n",
        "def translate_to_english(text, lang):\n",
        "    try:\n",
        "        if lang != \"en\" and lang != \"unknown\":\n",
        "            translator = Translator(to_lang=\"en\")\n",
        "            return translator.translate(text)\n",
        "        return text\n",
        "    except:\n",
        "        return text\n",
        "\n",
        "translate_udf = udf(translate_to_english, StringType())\n",
        "\n",
        "dfs_merged = dfs_merged.withColumn(\"title_translated\", translate_udf(col(\"Title\"), col(\"language_Title\")))\n",
        "\n",
        "dfs_merged.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMx251BhsSSF",
        "outputId": "b9cb3ea4-d891-46d4-db56-0a0131034cac"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "from langdetect import detect\n",
        "from translate import Translator\n",
        "\n",
        "spark = SparkSession.builder.appName(\"TitleTranslation\").getOrCreate()\n",
        "\n",
        "\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "detect_language_udf = udf(detect_language, StringType())\n",
        "\n",
        "dfs_merged = dfs_merged.withColumn(\"language_Abstract\", detect_language_udf(col(\"Abstract\")))\n",
        "\n",
        "def translate_to_english(text, lang):\n",
        "    try:\n",
        "        if lang != \"en\" and lang != \"unknown\":\n",
        "            translator = Translator(to_lang=\"en\")\n",
        "            return translator.translate(text)\n",
        "        return text\n",
        "    except:\n",
        "        return text\n",
        "\n",
        "translate_udf = udf(translate_to_english, StringType())\n",
        "\n",
        "dfs_merged = dfs_merged.withColumn(\"abstract_translated\", translate_udf(col(\"Abstract\"), col(\"language_Abstract\")))\n",
        "\n",
        "dfs_merged.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGphzJKd0O_e",
        "outputId": "a6c0bdf7-ed17-4d53-a1c9-1e27da74426f"
      },
      "outputs": [],
      "source": [
        "dfs_merged.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de2XPh910czU"
      },
      "outputs": [],
      "source": [
        "\n",
        "dfs_merged_final = dfs_merged.select('Title', 'Abstract', 'Inventors', 'Application_Date', 'country', 'Source','title_no_stopwords_fr', 'abstract_no_stopwords_fr', 'language_Title', 'title_translated','language_Abstract', 'abstract_translated')\n",
        "\n",
        "dfs_merged_final = dfs_merged_final.toPandas()\n",
        "nouvelles_colonnes = {\n",
        "    'title_no_stopwords_fr': 'title_tokens',\n",
        "    'abstract_no_stopwords_fr': 'abstract_tokens'\n",
        "}\n",
        "\n",
        "dfs_merged_final = dfs_merged_final.rename(columns=nouvelles_colonnes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "DMUTJqEC1JgU",
        "outputId": "730f0373-d5b6-452b-b7f2-54c8eef50bbe"
      },
      "outputs": [],
      "source": [
        "dfs_merged_final.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZVXRvyv1ftW"
      },
      "outputs": [],
      "source": [
        "\n",
        "csv_path = \"/content/df_merged_final.csv\"\n",
        "dfs_merged_final.to_csv(csv_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4Mo6Zb1NtK_"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.sql.functions import col, expr\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SimilarityCalculation\").getOrCreate()\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"title_translated\", outputCol=\"words\")\n",
        "tokenized_df = tokenizer.transform(dfs_merged)\n",
        "\n",
        "source_df_filtered = tokenized_df.filter(col(\"title_translated\").isNotNull())\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
        "tfidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "tfidf_model = tfidf.fit(hashingTF.transform(source_df_filtered))\n",
        "tfidf_df = tfidf_model.transform(hashingTF.transform(source_df_filtered))\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    dot_product = float(v1.dot(v2))\n",
        "    norm_product = float(v1.norm(2) * v2.norm(2))\n",
        "    similarity = dot_product / norm_product if norm_product != 0 else 0.0\n",
        "    return similarity\n",
        "\n",
        "cosine_similarity_udf = udf(cosine_similarity, FloatType())\n",
        "\n",
        "pairs = source_df_filtered.select(col(\"row_index\").alias(\"row_index1\")) \\\n",
        "    .crossJoin(source_df_filtered.select(col(\"row_index\").alias(\"row_index2\"))) \\\n",
        "    .where(\"row_index1 < row_index2\")\n",
        "\n",
        "similarity_df_1 = pairs.join(tfidf_df.alias(\"df1\"), col(\"row_index1\") == col(\"df1.row_index\")) \\\n",
        "    .join(tfidf_df.alias(\"df2\"), col(\"row_index2\") == col(\"df2.row_index\")) \\\n",
        "    .select(col(\"row_index1\"), col(\"row_index2\"), cosine_similarity_udf(col(\"df1.features\"), col(\"df2.features\")).alias(\"cosine_similarity\"))\n",
        "\n",
        "similarity_df_1.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ip3R5ijnsiWW"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
        "from pyspark.ml.linalg import SparseVector\n",
        "from pyspark.sql.functions import col, expr\n",
        "from itertools import combinations\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SimilarityCalculation\").getOrCreate()\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"abstract_translated\", outputCol=\"words\")\n",
        "tokenized_df = tokenizer.transform(dfs_merged)\n",
        "\n",
        "source_df_filtered = tokenized_df.filter(col(\"abstract_translated\").isNotNull())\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
        "tfidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "tfidf_model = tfidf.fit(hashingTF.transform(source_df_filtered))\n",
        "tfidf_df = tfidf_model.transform(hashingTF.transform(source_df_filtered))\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    dot_product = float(v1.dot(v2))\n",
        "    norm_product = float(v1.norm(2) * v2.norm(2))\n",
        "    similarity = dot_product / norm_product if norm_product != 0 else 0.0\n",
        "    return similarity\n",
        "\n",
        "cosine_similarity_udf = udf(cosine_similarity, FloatType())\n",
        "\n",
        "pairs = dfs_merged.select(col(\"row_index\").alias(\"row_index1\")).crossJoin(dfs_merged.select(col(\"row_index\").alias(\"row_index2\"))).where(\"row_index1 < row_index2\")\n",
        "\n",
        "similarity_df_2 = pairs.join(tfidf_df.alias(\"df1\"), col(\"row_index1\") == col(\"df1.row_index\")) \\\n",
        "    .join(tfidf_df.alias(\"df2\"), col(\"row_index2\") == col(\"df2.row_index\")) \\\n",
        "    .select(col(\"row_index1\"), col(\"row_index2\"), cosine_similarity_udf(col(\"df1.features\"), col(\"df2.features\")).alias(\"cosine_similarity\"))\n",
        "\n",
        "similarity_df_2.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0ZC22yvpC_q",
        "outputId": "25bde4ff-069c-4db7-b74e-06eef55e3d88"
      },
      "outputs": [],
      "source": [
        "\n",
        "subset_similarity_df = similarity_df_1.filter((col(\"row_index1\") >= 200) & (col(\"row_index1\") <= 220))\n",
        "\n",
        "subset_similarity_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqLY8OSHs6mz",
        "outputId": "02fc421a-ef66-4328-cce3-b6508195775d"
      },
      "outputs": [],
      "source": [
        "\n",
        "subset_similarity_df = similarity_df_2.filter((col(\"row_index1\") >= 200) & (col(\"row_index1\") <= 220))\n",
        "\n",
        "subset_similarity_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSXm9sUOoA9T",
        "outputId": "e529d6d6-8018-4470-b41a-9a074aed13e3"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_rows = similarity_df_1.count()\n",
        "\n",
        "print(\"Nombre de lignes dans similarity_df :\", num_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geMAmfbYs_lh",
        "outputId": "6b101911-339b-4d37-deb3-f13c26cfde0d"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_rows = similarity_df_2.count()\n",
        "\n",
        "print(\"Nombre de lignes dans similarity_df :\", num_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "W4wSEceWydgr"
      },
      "outputs": [],
      "source": [
        "\n",
        "similarity_df_2.write.csv('/content/similarity_abstract.csv', header=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
